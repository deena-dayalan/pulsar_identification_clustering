{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bope1tfuXuwT"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRqOTzsTI5bV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "from sklearn.metrics import classification_report, confusion_matrix, brier_score_loss, roc_curve, precision_recall_curve, silhouette_score, calinski_harabasz_score, davies_bouldin_score, recall_score, make_scorer, mean_squared_error\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i1oaLaQXXvx9"
   },
   "source": [
    "Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o22cNNQSiGP1"
   },
   "outputs": [],
   "source": [
    "pulsar_dataset = '/home/d.dasarathan/ds5500/projects/datasets/HTRU2/HTRU_2.csv'\n",
    "pulsar_data = np.loadtxt(pulsar_dataset, delimiter = ',', skiprows=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling(X_train, y_train):\n",
    "    X_minority = np.array([X_train[i] for i in range(len(X_train)) if y_train[i] == 1])\n",
    "    kappa = (1.0 * len(X_minority)) / len(X_train)\n",
    "    print('total training size: ', len(X_train))\n",
    "    print('positive class training size: ', len(X_minority))\n",
    "    repeat = int(1.0 / kappa) - 1\n",
    "    upsampling = resample(X_minority, n_samples=repeat * len(X_minority))\n",
    "    y_minority = np.ones(repeat * len(X_minority))\n",
    "    X_train = np.concatenate((X_train, upsampling))\n",
    "    print('updated positive class training size: ', len(upsampling))\n",
    "    y_train = np.concatenate((y_train, y_minority))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling_train(X_train, y_train):\n",
    "    X_minority = np.array([X_train[i] for i in range(len(X_train)) if y_train[i] == 1])\n",
    "    kappa = (1.0 * len(X_minority)) / len(X_train)\n",
    "    print('total training size: ', len(X_train))\n",
    "    print('positive class training size: ', len(X_minority))\n",
    "    repeat = int(1.0 / kappa) - 1\n",
    "    upsampling = resample(X_minority, n_samples=repeat * len(X_minority))\n",
    "    y_minority = np.ones(repeat * len(X_minority))\n",
    "    X_train = np.concatenate((X_train, upsampling))\n",
    "    print('updated positive class training size: ', len(upsampling))\n",
    "    y_train = np.concatenate((y_train, y_minority))\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIIRv4yVJF9D"
   },
   "outputs": [],
   "source": [
    "def data_prep(X, y, normalize=False, upsample=False, train_val_test=False, train_test=False, k_fold=False, k=None):\n",
    "  if normalize:\n",
    "    scaler = StandardScaler().fit(X, y)\n",
    "    X = scaler.transform(X)\n",
    "  if train_val_test:\n",
    "    X, X_test, y, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)\n",
    "    if upsample:\n",
    "      upsampling(X_train, y_train)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "  if train_test:\n",
    "    X, X_test, y, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    if k_fold:\n",
    "      data_sets = []\n",
    "      kf = KFold(n_splits=k, shuffle=True)\n",
    "      for train_index, val_index in kf.split(X, y):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        if upsample: \n",
    "          upsampling(X_train, y_train)\n",
    "        data_sets.append((X_train, X_val, y_train, y_val))\n",
    "      if upsample:\n",
    "        upsampling(X, y)\n",
    "      return data_sets, X, X_test, y, y_test\n",
    "    else:\n",
    "      if upsample: \n",
    "        upsampling(X, y)\n",
    "      return X, X_test, y, y_test\n",
    "  if k_fold:\n",
    "    data_sets = []\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "      X_train, X_test = X[train_index], X[test_index]\n",
    "      y_train, y_test = y[train_index], y[test_index]\n",
    "      if upsample: \n",
    "        upsampling(X_train, y_train)\n",
    "      data_sets.append((X_train, X_test, y_train, y_test))\n",
    "    return data_sets, X, y\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-eGt7BS3T59"
   },
   "source": [
    "Display & Plot Evaluation Metrics (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XQNdk1593UBj"
   },
   "outputs": [],
   "source": [
    "def eval_clf(clf_pred, clf_pred_prob, y_val, clf_score, alternative_treshold=None, plot=False):\n",
    "  print(\"score: \", clf_score)\n",
    "  print(classification_report(y_val, clf_pred))\n",
    "  cm = confusion_matrix(y_val, clf_pred)\n",
    "  print('Confusion matrix\\n', cm)\n",
    "  print('True Positives(TP) = ', cm[1,1])\n",
    "  print('True Negatives(TN) = ', cm[0,0])\n",
    "  print('False Positives(FP) = ', cm[0,1])\n",
    "  print('False Negatives(FN) = ', cm[1,0])\n",
    "\n",
    "  TP = cm[1,1]\n",
    "  TN = cm[0,0]\n",
    "  FP = cm[0,1]\n",
    "  FN = cm[1,0]\n",
    "  \n",
    "  ## classification accuracy\n",
    "  classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "  print('Classification accuracy : {0:0.4f}'.format(classification_accuracy))\n",
    "\n",
    "  ## classification error\n",
    "  classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "  print('Classification error : {0:0.4f}'.format(classification_error))\n",
    "\n",
    "  ## precision score\n",
    "  precision = TP / float(TP + FP)\n",
    "  print('Precision : {0:0.4f}'.format(precision))\n",
    "\n",
    "  ## recall \n",
    "  recall = TP / float(TP + FN)\n",
    "  print('Recall or Sensitivity : {0:0.4f}'.format(recall))\n",
    "\n",
    "  ## specificity\n",
    "  specificity = TN / (TN + FP)\n",
    "  print('Specificity : {0:0.4f}'.format(specificity))\n",
    "\n",
    "  ## F-1 score\n",
    "  f1 = 2 * (precision * recall) / (precision + recall)\n",
    "  print('F-1 score : {0:0.4f}'.format(f1))\n",
    "  \n",
    "  ROC_AUC = roc_auc_score(y_val, clf_pred)\n",
    "  print('ROC AUC : {:.4f}'.format(ROC_AUC))\n",
    "    \n",
    "  #print(\"brier score loss: \", brier_score_loss(y_val, clf_pred_prob[:,1]))\n",
    "\n",
    "  if plot:\n",
    "    clf_fpr, clf_tpr, clf_thresholds = roc_curve(y_val, clf_pred_prob[:,1])\n",
    "    plt.plot(clf_fpr, clf_tpr, marker='.')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('1 - Specifity (FPR)')\n",
    "    plt.ylabel('Sensitivity (TPR)')\n",
    "    plt.show()\n",
    "    plt.plot(clf_thresholds, clf_tpr, marker='.', label='Sensitivity (TPR)')\n",
    "    plt.plot(clf_thresholds, clf_fpr, marker='.', label='1 - Specifity (FPR)')\n",
    "    plt.plot(clf_thresholds, clf_tpr - clf_fpr, marker='.', label='Difference (TPR - FPR)')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Sensitivity / 1 - Specifity / Difference vs. Threshold')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Sensitivity / 1 - Specifity / Difference')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    clf_precision, clf_recall, clf_thresholds = precision_recall_curve(y_val, clf_pred_prob[:,1])\n",
    "    plt.plot(clf_recall, clf_precision, marker='.')\n",
    "    plt.title('PRC Curve')\n",
    "    plt.xlabel('Recall (TPR)')\n",
    "    plt.ylabel('Precision (PPV)')\n",
    "    plt.show()\n",
    "\n",
    "    clf_frac_pos, clf_prob_pos = calibration_curve(y_val, clf_pred_prob[:,1], n_bins=10, strategy='uniform')\n",
    "    plt.plot(clf_prob_pos, clf_frac_pos, marker='.')\n",
    "    plt.title('Calibration')\n",
    "    plt.xlabel('Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.show()\n",
    "\n",
    "  if alternative_treshold:\n",
    "    print(\"Threshold = \", alternative_treshold, \":\")\n",
    "    clf_new_pred = [1 * (clf_pred_sample_prob[1] > alternative_treshold) for clf_pred_sample_prob in clf_pred_prob]\n",
    "    print(classification_report(y_val, clf_new_pred))\n",
    "    print(\"confusion matrix:\")\n",
    "    print(confusion_matrix(y_val, clf_new_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAkj8IB-I6xZ"
   },
   "source": [
    "Guassian Discriminant Analysis (Baseline)\n",
    "(using scikit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wGqPsy4dIBD3"
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "def gda(X_train, X_val, y_train, y_val):\n",
    "  clf = QuadraticDiscriminantAnalysis()\n",
    "  clf.fit(X_train, y_train)\n",
    "  gda_pred = clf.predict(X_val)\n",
    "  gda_pred_prob = clf.predict_proba(X_val)\n",
    "  gda_score = clf.score(X_val, y_val)\n",
    "  gda_train_error = mean_squared_error(y_train, clf.predict(X_train))\n",
    "  gda_val_error = mean_squared_error(y_val, gda_pred)\n",
    "  return gda_pred, gda_pred_prob, gda_score, gda_train_error, gda_val_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7k4A7dqsz4I"
   },
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "30HoZmDXsz_Q",
    "outputId": "d1859e83-6f44-4c6d-c291-a460fa825907"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (17898, 8)\n",
      "y shape :  (17898,)\n",
      "X_train shape :  (14318, 8)\n",
      "X_test shape :  (3580, 8)\n",
      "GDA - Test Set performance:\n",
      "recall:  0.8640226628895185\n",
      "score:  0.9675977653631285\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.98      3227\n",
      "         1.0       0.82      0.86      0.84       353\n",
      "\n",
      "    accuracy                           0.97      3580\n",
      "   macro avg       0.90      0.92      0.91      3580\n",
      "weighted avg       0.97      0.97      0.97      3580\n",
      "\n",
      "Confusion matrix\n",
      " [[3159   68]\n",
      " [  48  305]]\n",
      "True Positives(TP) =  305\n",
      "True Negatives(TN) =  3159\n",
      "False Positives(FP) =  68\n",
      "False Negatives(FN) =  48\n",
      "Classification accuracy : 0.9676\n",
      "Classification error : 0.0324\n",
      "Precision : 0.8177\n",
      "Recall or Sensitivity : 0.8640\n",
      "Specificity : 0.9789\n",
      "F-1 score : 0.8402\n",
      "ROC AUC : 0.9215\n",
      "============================================================================================\n",
      "X shape :  (17898, 8)\n",
      "y shape :  (17898,)\n",
      "X_train shape :  (14318, 8)\n",
      "X_test shape :  (3580, 8)\n",
      "total training size:  14318\n",
      "positive class training size:  1298\n",
      "updated positive class training size:  12980\n",
      "X_train shape :  (27298, 8)\n",
      "y_train shape :  (27298,)\n",
      "X_test shape :  (3580, 8)\n",
      "y_test shape :  (3580,)\n",
      "Train data class distribution\n",
      "[[    0. 13020.]\n",
      " [    1. 14278.]]\n",
      "Test data class distribution\n",
      "[[   0. 3239.]\n",
      " [   1.  341.]]\n",
      "GDA - Upsampled - Test Set Performance:\n",
      "recall:  0.8973607038123167\n",
      "score:  0.9653631284916201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.97      0.98      3239\n",
      "         1.0       0.77      0.90      0.83       341\n",
      "\n",
      "    accuracy                           0.97      3580\n",
      "   macro avg       0.88      0.93      0.91      3580\n",
      "weighted avg       0.97      0.97      0.97      3580\n",
      "\n",
      "Confusion matrix\n",
      " [[3150   89]\n",
      " [  35  306]]\n",
      "True Positives(TP) =  306\n",
      "True Negatives(TN) =  3150\n",
      "False Positives(FP) =  89\n",
      "False Negatives(FN) =  35\n",
      "Classification accuracy : 0.9654\n",
      "Classification error : 0.0346\n",
      "Precision : 0.7747\n",
      "Recall or Sensitivity : 0.8974\n",
      "Specificity : 0.9725\n",
      "F-1 score : 0.8315\n",
      "ROC AUC : 0.9349\n"
     ]
    }
   ],
   "source": [
    "X, y = pulsar_data[:,:-1], pulsar_data[:,-1]\n",
    "print(\"X shape : \",X.shape )\n",
    "print(\"y shape : \",y.shape )\n",
    "X_train, X_test, y_train, y_test = data_prep(X, y, train_test=True, upsample=False)\n",
    "print(\"X_train shape : \",X_train.shape )\n",
    "print(\"X_test shape : \",X_test.shape )\n",
    "\n",
    "print(\"GDA - Test Set performance:\")\n",
    "gda_pred, gda_pred_prob, gda_score, gda_train_error, gda_val_error = gda(X_train, X_test, y_train, y_test)\n",
    "print(\"recall: \", recall_score(y_test, gda_pred))\n",
    "eval_clf(gda_pred, gda_pred_prob, y_test, gda_score, plot=False)\n",
    "\n",
    "print(\"============================================================================================\")\n",
    "\n",
    "X, y = pulsar_data[:,:-1], pulsar_data[:,-1]\n",
    "print(\"X shape : \",X.shape )\n",
    "print(\"y shape : \",y.shape )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(\"X_train shape : \",X_train.shape )\n",
    "print(\"X_test shape : \",X_test.shape )\n",
    "\n",
    "X_train, y_train = upsampling_train(X_train, y_train)\n",
    "print(\"X_train shape : \",X_train.shape)\n",
    "print(\"y_train shape : \",y_train.shape)\n",
    "print(\"X_test shape : \",X_test.shape)\n",
    "print(\"y_test shape : \",y_test.shape)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Train data class distribution\")\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"Test data class distribution\")\n",
    "print(np.asarray((unique, counts)).T)\n",
    "\n",
    "print(\"GDA - Upsampled - Test Set Performance:\")\n",
    "gda_pred, gda_pred_prob, gda_score, gda_train_error, gda_test_error = gda(X_train, X_test, y_train, y_test)\n",
    "print(\"recall: \", recall_score(y_test, gda_pred))\n",
    "eval_clf(gda_pred, gda_pred_prob, y_test, gda_score, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GCm-7xRHLc_s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS 229 - Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
